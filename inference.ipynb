{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "import librosa\n",
    "from librosa.core import load\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "mel_basis = librosa_mel_fn(22050, 1024, 80, 0, 8000)\n",
    "\n",
    "import params\n",
    "from model import DiffVC\n",
    "\n",
    "import sys\n",
    "sys.path.append('hifi-gan/')\n",
    "from env import AttrDict\n",
    "from models import Generator as HiFiGAN\n",
    "\n",
    "sys.path.append('speaker_encoder/')\n",
    "from encoder import inference as spk_encoder\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel(wav_path):\n",
    "    wav, _ = load(wav_path, sr=22050)\n",
    "    wav = wav[:(wav.shape[0] // 256)*256]\n",
    "    wav = np.pad(wav, 384, mode='reflect')\n",
    "    stft = librosa.core.stft(wav, n_fft=1024, hop_length=256, win_length=1024, window='hann', center=False)\n",
    "    stftm = np.sqrt(np.real(stft) ** 2 + np.imag(stft) ** 2 + (1e-9))\n",
    "    mel_spectrogram = np.matmul(mel_basis, stftm)\n",
    "    log_mel_spectrogram = np.log(np.clip(mel_spectrogram, a_min=1e-5, a_max=None))\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def get_embed(wav_path):\n",
    "    wav_preprocessed = spk_encoder.preprocess_wav(wav_path)\n",
    "    embed = spk_encoder.embed_utterance(wav_preprocessed)\n",
    "    return embed\n",
    "\n",
    "def noise_median_smoothing(x, w=5):\n",
    "    y = np.copy(x)\n",
    "    x = np.pad(x, w, \"edge\")\n",
    "    for i in range(y.shape[0]):\n",
    "        med = np.median(x[i:i+2*w+1])\n",
    "        y[i] = min(x[i+w+1], med)\n",
    "    return y\n",
    "\n",
    "def mel_spectral_subtraction(mel_synth, mel_source, spectral_floor=0.02, silence_window=5, smoothing_window=5):\n",
    "    mel_len = mel_source.shape[-1]\n",
    "    energy_min = 100000.0\n",
    "    i_min = 0\n",
    "    for i in range(mel_len - silence_window):\n",
    "        energy_cur = np.sum(np.exp(2.0 * mel_source[:, i:i+silence_window]))\n",
    "        if energy_cur < energy_min:\n",
    "            i_min = i\n",
    "            energy_min = energy_cur\n",
    "    estimated_noise_energy = np.min(np.exp(2.0 * mel_synth[:, i_min:i_min+silence_window]), axis=-1)\n",
    "    if smoothing_window is not None:\n",
    "        estimated_noise_energy = noise_median_smoothing(estimated_noise_energy, smoothing_window)\n",
    "    mel_denoised = np.copy(mel_synth)\n",
    "    for i in range(mel_len):\n",
    "        signal_subtract_noise = np.exp(2.0 * mel_synth[:, i]) - estimated_noise_energy\n",
    "        estimated_signal_energy = np.maximum(signal_subtract_noise, spectral_floor * estimated_noise_energy)\n",
    "        mel_denoised[:, i] = np.log(np.sqrt(estimated_signal_energy))\n",
    "    return mel_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading voice conversion model\n",
    "vc_path = 'checkpts/vc/vc_libritts_wodyn.pt' # path to voice conversion model\n",
    "\n",
    "generator = DiffVC(params.n_mels, params.channels, params.filters, params.heads, \n",
    "                   params.layers, params.kernel, params.dropout, params.window_size, \n",
    "                   params.enc_dim, params.spk_dim, params.use_ref_t, params.dec_dim, \n",
    "                   params.beta_min, params.beta_max)\n",
    "if use_gpu:\n",
    "    generator = generator.cuda()\n",
    "    generator.load_state_dict(torch.load(vc_path))\n",
    "else:\n",
    "    generator.load_state_dict(torch.load(vc_path, map_location='cpu'))\n",
    "generator.eval()\n",
    "\n",
    "print(f'Number of parameters: {generator.nparams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading HiFi-GAN vocoder\n",
    "hfg_path = 'checkpts/vocoder/' # HiFi-GAN path\n",
    "\n",
    "with open(hfg_path + 'config.json') as f:\n",
    "    h = AttrDict(json.load(f))\n",
    "\n",
    "if use_gpu:\n",
    "    hifigan_universal = HiFiGAN(h).cuda()\n",
    "    hifigan_universal.load_state_dict(torch.load(hfg_path + 'generator')['generator'])\n",
    "else:\n",
    "    hifigan_universal = HiFiGAN(h)\n",
    "    hifigan_universal.load_state_dict(torch.load(hfg_path + 'generator',  map_location='cpu')['generator'])\n",
    "\n",
    "_ = hifigan_universal.eval()\n",
    "hifigan_universal.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading speaker encoder\n",
    "enc_model_fpath = Path('checkpts/spk_encoder/pretrained.pt') # speaker encoder path\n",
    "if use_gpu:\n",
    "    spk_encoder.load_model(enc_model_fpath, device=\"cuda\")\n",
    "else:\n",
    "    spk_encoder.load_model(enc_model_fpath, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading source and reference wavs, calculating mel-spectrograms and speaker embeddings\n",
    "src_path = 'example/6415_111615_000012_000005.wav' # path to source utterance\n",
    "tgt_path = 'example/8534_216567_000015_000010.wav' # path to reference utterance\n",
    "\n",
    "mel_source = torch.from_numpy(get_mel(src_path)).float().unsqueeze(0)\n",
    "if use_gpu:\n",
    "    mel_source = mel_source.cuda()\n",
    "mel_source_lengths = torch.LongTensor([mel_source.shape[-1]])\n",
    "if use_gpu:\n",
    "    mel_source_lengths = mel_source_lengths.cuda()\n",
    "\n",
    "mel_target = torch.from_numpy(get_mel(tgt_path)).float().unsqueeze(0)\n",
    "if use_gpu:\n",
    "    mel_target = mel_target.cuda()\n",
    "mel_target_lengths = torch.LongTensor([mel_target.shape[-1]])\n",
    "if use_gpu:\n",
    "    mel_target_lengths = mel_target_lengths.cuda()\n",
    "\n",
    "embed_target = torch.from_numpy(get_embed(tgt_path)).float().unsqueeze(0)\n",
    "if use_gpu:\n",
    "    embed_target = embed_target.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing voice conversion\n",
    "mel_encoded, mel_ = generator.forward(mel_source, mel_source_lengths, mel_target, mel_target_lengths, embed_target, \n",
    "                                      n_timesteps=30, mode='ml')\n",
    "mel_synth_np = mel_.cpu().detach().squeeze().numpy()\n",
    "mel_source_np = mel_.cpu().detach().squeeze().numpy()\n",
    "mel = torch.from_numpy(mel_spectral_subtraction(mel_synth_np, mel_source_np, smoothing_window=1)).float().unsqueeze(0)\n",
    "if use_gpu:\n",
    "    mel = mel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source utterance (vocoded)\n",
    "with torch.no_grad():\n",
    "    audio = hifigan_universal.forward(mel_source).cpu().squeeze().clamp(-1, 1)\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference utterance (vocoded)\n",
    "with torch.no_grad():\n",
    "    audio = hifigan_universal.forward(mel_target).cpu().squeeze().clamp(-1, 1)\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted speech\n",
    "with torch.no_grad():\n",
    "    audio = hifigan_universal.forward(mel).cpu().squeeze().clamp(-1, 1)\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e9505f314c104ceeb86e0ca7b51d1dfd7e393d693f91505ea4ef9fb69fc7c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
